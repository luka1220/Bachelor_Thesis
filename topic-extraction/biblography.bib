
@article{jagarlamudi_incorporating_nodate,
	title = {Incorporating {Lexical} {Priors} into {Topic} {Models}},
	abstract = {Topic models have great potential for helping users understand document corpora. This potential is stymied by their purely unsupervised nature, which often leads to topics that are neither entirely meaningful nor effective in extrinsic tasks (Chang et al., 2009). We propose a simple and effective way to guide topic models to learn topics of speciﬁc interest to a user. We achieve this by providing sets of seed words that a user believes are representative of the underlying topics in a corpus. Our model uses these seeds to improve both topicword distributions (by biasing topics to produce appropriate seed words) and to improve document-topic distributions (by biasing documents to select topics related to the seed words they contain). Extrinsic evaluation on a document clustering task reveals a signiﬁcant improvement when using seed information, even over other models that use seed information na¨ıvely.},
	language = {en},
	author = {Jagarlamudi, Jagadeesh and Iii, Hal Daume and Udupa, Raghavendra},
	pages = {10},
	file = {Jagarlamudi et al. - Incorporating Lexical Priors into Topic Models.pdf:/Users/lukastaerk/Zotero/storage/X47SRW2B/Jagarlamudi et al. - Incorporating Lexical Priors into Topic Models.pdf:application/pdf}
}

@inproceedings{banerjee_clustering_2007,
	address = {Amsterdam, The Netherlands},
	title = {Clustering short texts using wikipedia},
	isbn = {978-1-59593-597-7},
	url = {http://portal.acm.org/citation.cfm?doid=1277741.1277909},
	doi = {10.1145/1277741.1277909},
	abstract = {Subscribers to the popular news or blog feeds (RSS/Atom) often face the problem of information overload as these feed sources usually deliver large number of items periodically. One solution to this problem could be clustering similar items in the feed reader to make the information more manageable for a user. Clustering items at the feed reader end is a challenging task as usually only a small part of the actual article is received through the feed. In this paper, we propose a method of improving the accuracy of clustering short texts by enriching their representation with additional features from Wikipedia. Empirical results indicate that this enriched representation of text items can substantially improve the clustering accuracy when compared to the conventional bag of words representation.},
	language = {en},
	urldate = {2019-06-12},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '07},
	publisher = {ACM Press},
	author = {Banerjee, Somnath and Ramanathan, Krishnan and Gupta, Ajay},
	year = {2007},
	pages = {787},
	file = {Banerjee et al. - 2007 - Clustering short texts using wikipedia.pdf:/Users/lukastaerk/Zotero/storage/JQD8NJKR/Banerjee et al. - 2007 - Clustering short texts using wikipedia.pdf:application/pdf}
}

@inproceedings{hu_exploiting_2009,
	address = {Hong Kong, China},
	title = {Exploiting internal and external semantics for the clustering of short texts using world knowledge},
	isbn = {978-1-60558-512-3},
	url = {http://portal.acm.org/citation.cfm?doid=1645953.1646071},
	doi = {10.1145/1645953.1646071},
	abstract = {Clustering of short texts, such as snippets, presents great challenges in existing aggregated search techniques due to the problem of data sparseness and the complex semantics of natural language. As short texts do not provide suﬃcient term co-occurrence information, traditional text representation methods, such as “bag of words” model, have several limitations when directly applied to short text tasks. In this paper, we propose a novel framework to improve the performance of short text clustering by exploiting the internal semantics from the original text and external concepts from world knowledge. The proposed method employs a hierarchical three-level structure to tackle the data sparsity problem of original short texts and reconstruct the corresponding feature space with the integration of multiple semantic knowledge bases – Wikipedia and WordNet. Empirical evaluation with Reuters and real web dataset demonstrates that our approach is able to achieve signiﬁcant improvement as compared to the state-of-the-art methods.},
	language = {en},
	urldate = {2019-06-12},
	booktitle = {Proceeding of the 18th {ACM} conference on {Information} and knowledge management - {CIKM} '09},
	publisher = {ACM Press},
	author = {Hu, Xia and Sun, Nan and Zhang, Chao and Chua, Tat-Seng},
	year = {2009},
	pages = {919},
	file = {Hu et al. - 2009 - Exploiting internal and external semantics for the.pdf:/Users/lukastaerk/Zotero/storage/MWEWHJ9G/Hu et al. - 2009 - Exploiting internal and external semantics for the.pdf:application/pdf}
}

@article{blei_latent_nodate,
	title = {Latent {Dirichlet} {Allocation}},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	language = {en},
	author = {Blei, David M},
	pages = {30},
	file = {Blei - Latent Dirichlet Allocation.pdf:/Users/lukastaerk/Zotero/storage/GZ6V9H2X/Blei - Latent Dirichlet Allocation.pdf:application/pdf}
}

@article{ganesan_opinosis:_nodate,
	title = {Opinosis: {A} {Graph} {Based} {Approach} to {Abstractive} {Summarization} of {Highly} {Redundant} {Opinions}},
	abstract = {We present a novel graph-based summarization framework (Opinosis) that generates concise abstractive summaries of highly redundant opinions. Evaluation results on summarizing user reviews show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method. The summaries are readable, reasonably well-formed and are informative enough to convey the major opinions.},
	language = {en},
	author = {Ganesan, Kavita and Zhai, ChengXiang and Han, Jiawei},
	pages = {9},
	file = {Ganesan et al. - Opinosis A Graph Based Approach to Abstractive Su.pdf:/Users/lukastaerk/Zotero/storage/NWWU2TPQ/Ganesan et al. - Opinosis A Graph Based Approach to Abstractive Su.pdf:application/pdf}
}